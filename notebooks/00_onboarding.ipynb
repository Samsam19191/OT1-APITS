{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Onboarding Notebook: Anticipatory Prefill Project\n",
                "\n",
                "This notebook verifies your environment is ready for development.\n",
                "It checks:\n",
                "1. Python & GPU\n",
                "2. Installs dependencies (handling both local and remote execution)\n",
                "3. Loads the project config & model\n",
                "4. Runs a basic inference\n",
                "5. Runs a KV-cache sanity check"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Runtime Checks & Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "9a0a5ccd",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Python Version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
                        "GPU Available: Tesla T4\n",
                        "CUDA Version: 12.6\n"
                    ]
                }
            ],
            "source": [
                "import sys\n",
                "import os\n",
                "import torch\n",
                "\n",
                "print(f\"Python Version: {sys.version}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU Available: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
                "else:\n",
                "    print(\"WARNING: No GPU detected. Please enable GPU runtime.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "89fcce01",
            "metadata": {},
            "source": [
                "## 2. Remote Setup (Colab/VS Code Remote)\n",
                "If running remotely (e.g. VS Code connected to Colab), we clone the repo into a subdirectory to ensure we have access to `src/`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "8fc21d48",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Local src not found. Checking for clone in OT1-APITS...\n",
                        "Repo already cloned in OT1-APITS. Pulling latest changes...\n",
                        "remote: Enumerating objects: 19, done.\u001b[K\n",
                        "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
                        "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
                        "remote: Total 17 (delta 0), reused 17 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
                        "Unpacking objects: 100% (17/17), 12.78 KiB | 4.26 MiB/s, done.\n",
                        "From https://github.com/Samsam19191/OT1-APITS\n",
                        "   3fe9f43..887f09b  main       -> origin/main\n",
                        "Updating 3fe9f43..887f09b\n",
                        "Fast-forward\n",
                        " .gitignore                                         |   5 \u001b[32m+\u001b[m\n",
                        " README.md                                          |  40 \u001b[32m++\u001b[m\u001b[31m-\u001b[m\n",
                        " docs/colab.md                                      |  19 \u001b[32m++\u001b[m\n",
                        " docs/vscode_colab.md                               |  24 \u001b[32m++\u001b[m\n",
                        " notebooks/00_onboarding.ipynb                      | 355 \u001b[32m+++++++++++++++++++++\u001b[m\n",
                        " experiments/requirements.txt => requirements.txt   |   4 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
                        " src/__init__.py                                    |   7 \u001b[32m+\u001b[m\n",
                        " src/config.py                                      |   7 \u001b[32m+\u001b[m\n",
                        " .../01_sanity_check.py                             |   0\n",
                        " .../02_extension.py                                |   0\n",
                        " .../03_cropping.py                                 |   0\n",
                        " .../model_validation_report.md                     |   0\n",
                        " {experiments => validation_experiments}/utils.py   |   0\n",
                        " 13 files changed, 457 insertions(+), 4 deletions(-)\n",
                        " create mode 100644 .gitignore\n",
                        " create mode 100644 docs/colab.md\n",
                        " create mode 100644 docs/vscode_colab.md\n",
                        " create mode 100644 notebooks/00_onboarding.ipynb\n",
                        " rename experiments/requirements.txt => requirements.txt (80%)\n",
                        " create mode 100644 src/__init__.py\n",
                        " create mode 100644 src/config.py\n",
                        " rename {experiments => validation_experiments}/01_sanity_check.py (100%)\n",
                        " rename {experiments => validation_experiments}/02_extension.py (100%)\n",
                        " rename {experiments => validation_experiments}/03_cropping.py (100%)\n",
                        " rename model_validation_report.md => validation_experiments/model_validation_report.md (100%)\n",
                        " rename {experiments => validation_experiments}/utils.py (100%)\n",
                        "Repository root set to: OT1-APITS\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "\n",
                "REPO_DIR = \"OT1-APITS\"\n",
                "\n",
                "# Check if we are running locally (src exists relative to us) or need to clone\n",
                "if os.path.exists(\"../src\"):\n",
                "    print(\"Running locally (parent directory has src).\")\n",
                "    REPO_ROOT = \"..\"\n",
                "elif os.path.exists(\"src\"):\n",
                "    print(\"Running locally (current directory has src).\")\n",
                "    REPO_ROOT = \".\"\n",
                "else:\n",
                "    # We are likely in a fresh Colab VM\n",
                "    print(f\"Local src not found. Checking for clone in {REPO_DIR}...\")\n",
                "    if not os.path.exists(REPO_DIR):\n",
                "        print(\"Cloning repo...\")\n",
                "        !git clone https://github.com/Samsam19191/OT1-APITS.git {REPO_DIR}\n",
                "    else:\n",
                "        print(f\"Repo already cloned in {REPO_DIR}. Pulling latest changes...\")\n",
                "        !cd {REPO_DIR} && git pull\n",
                "    \n",
                "    REPO_ROOT = REPO_DIR\n",
                "\n",
                "print(f\"Repository root set to: {REPO_ROOT}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2374d94b",
            "metadata": {},
            "source": [
                "## 3. Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "id": "25bbc2d0",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Found requirements at: OT1-APITS/requirements.txt\n",
                        "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from -r OT1-APITS/requirements.txt (line 1)) (2.9.0+cu126)\n",
                        "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from -r OT1-APITS/requirements.txt (line 2)) (4.57.3)\n",
                        "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (from -r OT1-APITS/requirements.txt (line 3)) (1.12.0)\n",
                        "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (from -r OT1-APITS/requirements.txt (line 4)) (0.49.0)\n",
                        "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from -r OT1-APITS/requirements.txt (line 5)) (0.2.1)\n",
                        "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from -r OT1-APITS/requirements.txt (line 6)) (5.29.5)\n",
                        "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->-r OT1-APITS/requirements.txt (line 1)) (3.20.0)\n",
                        "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r OT1-APITS/requirements.txt (line 1)) (4.15.0)\n",
                        "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->-r OT1-APITS/requirements.txt (line 1)) (75.2.0)\n",
                        "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r OT1-APITS/requirements.txt (line 1)) (1.14.0)\n",
                        "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r OT1-APITS/requirements.txt (line 1)) (3.6.1)\n",
                        "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->-r OT1-APITS/requirements.txt (line 1)) (3.1.6)\n",
                        "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->-r OT1-APITS/requirements.txt (line 1)) (2025.3.0)\n",
                        "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r OT1-APITS/requirements.txt (line 1)) (12.6.77)\n",
                        "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r OT1-APITS/requirements.txt (line 1)) (12.6.77)\n",
                        "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->-r OT1-APITS/requirements.txt (line 1)) (12.6.80)\n",
                        "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->-r OT1-APITS/requirements.txt (line 1)) (9.10.2.21)\n",
                        "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r OT1-APITS/requirements.txt (line 1)) (12.6.4.1)\n",
                        "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->-r OT1-APITS/requirements.txt (line 1)) (11.3.0.4)\n",
                        "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r OT1-APITS/requirements.txt (line 1)) (10.3.7.77)\n",
                        "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r OT1-APITS/requirements.txt (line 1)) (11.7.1.2)\n",
                        "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r OT1-APITS/requirements.txt (line 1)) (12.5.4.2)\n",
                        "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r OT1-APITS/requirements.txt (line 1)) (0.7.1)\n",
                        "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->-r OT1-APITS/requirements.txt (line 1)) (2.27.5)\n",
                        "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->-r OT1-APITS/requirements.txt (line 1)) (3.3.20)\n",
                        "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r OT1-APITS/requirements.txt (line 1)) (12.6.77)\n",
                        "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->-r OT1-APITS/requirements.txt (line 1)) (12.6.85)\n",
                        "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->-r OT1-APITS/requirements.txt (line 1)) (1.11.1.6)\n",
                        "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r OT1-APITS/requirements.txt (line 1)) (3.5.0)\n",
                        "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers->-r OT1-APITS/requirements.txt (line 2)) (0.36.0)\n",
                        "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers->-r OT1-APITS/requirements.txt (line 2)) (2.0.2)\n",
                        "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers->-r OT1-APITS/requirements.txt (line 2)) (25.0)\n",
                        "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers->-r OT1-APITS/requirements.txt (line 2)) (6.0.3)\n",
                        "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->-r OT1-APITS/requirements.txt (line 2)) (2025.11.3)\n",
                        "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers->-r OT1-APITS/requirements.txt (line 2)) (2.32.4)\n",
                        "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->-r OT1-APITS/requirements.txt (line 2)) (0.22.1)\n",
                        "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers->-r OT1-APITS/requirements.txt (line 2)) (0.7.0)\n",
                        "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers->-r OT1-APITS/requirements.txt (line 2)) (4.67.1)\n",
                        "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate->-r OT1-APITS/requirements.txt (line 3)) (5.9.5)\n",
                        "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers->-r OT1-APITS/requirements.txt (line 2)) (1.2.0)\n",
                        "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->-r OT1-APITS/requirements.txt (line 1)) (1.3.0)\n",
                        "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->-r OT1-APITS/requirements.txt (line 1)) (3.0.3)\n",
                        "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->-r OT1-APITS/requirements.txt (line 2)) (3.4.4)\n",
                        "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->-r OT1-APITS/requirements.txt (line 2)) (3.11)\n",
                        "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->-r OT1-APITS/requirements.txt (line 2)) (2.5.0)\n",
                        "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->-r OT1-APITS/requirements.txt (line 2)) (2025.11.12)\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "\n",
                "req_file = os.path.join(REPO_ROOT, \"requirements.txt\")\n",
                "\n",
                "if os.path.exists(req_file):\n",
                "    print(f\"Found requirements at: {req_file}\")\n",
                "    !pip install -r {req_file}\n",
                "else:\n",
                "    print(f\"WARNING: requirements.txt not found at {req_file}\")\n",
                "    print(\"Current working directory files:\", os.listdir(\".\"))\n",
                "    if os.path.exists(REPO_ROOT):\n",
                "        print(f\"Files in {REPO_ROOT}:\", os.listdir(REPO_ROOT))\n",
                "        \n",
                "    print(\"Installing default packages as fallback...\")\n",
                "    !pip install torch transformers accelerate bitsandbytes sentencepiece protobuf"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Import & Load Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Config loaded from src. Model: Qwen/Qwen2.5-1.5B-Instruct\n",
                        "Transformers version: 4.57.3\n",
                        "Loading model: Qwen/Qwen2.5-1.5B-Instruct...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
                        "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
                        "You are not authenticated with the Hugging Face Hub in this notebook.\n",
                        "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "c475a47f629949baa8b715a1c3374f9a",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer_config.json: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "55c0a6eee4884908a3c23c8e41eb9969",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "vocab.json: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "67061196d66242608d25f1bb6a33e8e2",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "merges.txt: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "1e704e3d1deb46c99bdca227604bf7a1",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer.json: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "e88c71b899bb4fd7a7d8e8815e5e61ba",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "config.json:   0%|          | 0.00/660 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "1c011a602a694f26b9011d1107ebdc3c",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "368547b10aea4553a2f1380ab8834850",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model loaded successfully!\n"
                    ]
                }
            ],
            "source": [
                "import sys\n",
                "import os\n",
                "\n",
                "# Add the repo root key paths to sys.path so imports work\n",
                "if REPO_ROOT not in sys.path:\n",
                "    sys.path.append(os.path.abspath(REPO_ROOT))\n",
                "\n",
                "import transformers\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
                "\n",
                "try:\n",
                "    # Try importing from src.config\n",
                "    from src.config import MODEL_NAME, LOAD_IN_4BIT, DEVICE, MAX_SEQ_LEN_TYPING\n",
                "    print(f\"Config loaded from src. Model: {MODEL_NAME}\")\n",
                "except ImportError as e:\n",
                "    print(f\"ImportError: {e}\")\n",
                "    print(\"Could not import src.config. Using defaults.\")\n",
                "    MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
                "    LOAD_IN_4BIT = True\n",
                "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "\n",
                "print(f\"Transformers version: {transformers.__version__}\")\n",
                "print(f\"Loading model: {MODEL_NAME}...\")\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    MODEL_NAME,\n",
                "    device_map=\"auto\",\n",
                "    load_in_4bit=LOAD_IN_4BIT,\n",
                "    trust_remote_code=True\n",
                ")\n",
                "\n",
                "print(\"Model loaded successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Sanity Inference"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- Generation sanity check ---\n",
                        "def fibonacci(n): \n",
                        "    a = 0\n",
                        "    b = 1\n",
                        "    if n == 0:\n",
                        "\n",
                        "------------------------------\n"
                    ]
                }
            ],
            "source": [
                "input_text = \"def fibonacci(n):\"\n",
                "inputs = tokenizer(input_text, return_tensors=\"pt\").to(DEVICE)\n",
                "\n",
                "outputs = model.generate(**inputs, max_new_tokens=20)\n",
                "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "\n",
                "print(\"--- Generation sanity check ---\")\n",
                "print(result)\n",
                "print(\"------------------------------\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. KV-Cache Sanity Check"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "✅ KV Cache extracted successfully.\n",
                        "Cache type: <class 'transformers.cache_utils.DynamicCache'>\n",
                        "Sequence length in cache: 4\n"
                    ]
                }
            ],
            "source": [
                "prompt = \"The quick brown fox\"\n",
                "inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
                "\n",
                "# Run forward pass with use_cache=True\n",
                "with torch.no_grad():\n",
                "    outputs = model(inputs.input_ids, use_cache=True)\n",
                "\n",
                "kv_cache = outputs.past_key_values\n",
                "\n",
                "if kv_cache is not None:\n",
                "    print(\"✅ KV Cache extracted successfully.\")\n",
                "    # Check structure (layers, keys/values)\n",
                "    print(f\"Cache type: {type(kv_cache)}\")\n",
                "    if hasattr(kv_cache, \"get_seq_length\"):\n",
                "         print(f\"Sequence length in cache: {kv_cache.get_seq_length()}\")\n",
                "    else:\n",
                "         # Fallback for old tuple-based cache\n",
                "         print(f\"Layers: {len(kv_cache)}\")\n",
                "else:\n",
                "    print(\"❌ Failed to extract KV Cache.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
