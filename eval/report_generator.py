"""
Report Generator - Creates markdown evaluation reports.
"""

import os
from datetime import datetime
from typing import List
from pathlib import Path

from metrics import Metrics, EvalResult


def generate_report(metrics: Metrics, results: List[EvalResult]) -> str:
    """Generate markdown report and return file path."""
    
    # Ensure reports directory exists
    reports_dir = Path(__file__).parent / "data" / "reports"
    reports_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_path = reports_dir / f"eval_{timestamp}.md"
    
    # Generate content
    content = _build_report(metrics, results)
    
    with open(report_path, "w") as f:
        f.write(content)
    
    return str(report_path)


def _build_report(metrics: Metrics, results: List[EvalResult]) -> str:
    """Build the markdown content."""
    
    # Status emoji
    acc = metrics.exact_match_accuracy
    status = "üü¢ Excellent" if acc >= 80 else "üü° Good" if acc >= 60 else "üü† Fair" if acc >= 40 else "üî¥ Poor"
    
    # Failures
    failures = [r for r in results if not r.is_correct()]
    syntax_errors = [r for r in failures if not r.syntax_valid]
    wrong_results = [r for r in failures if r.syntax_valid and not r.result_match]
    
    # Build report
    report = f"""# SQL Evaluation Report

**Model**: {metrics.model_name}  
**Date**: {metrics.timestamp}  
**Total Cases**: {metrics.total}

## Summary

{status}

| Metric | Value |
|--------|-------|
| Execution Accuracy | {metrics.execution_accuracy:.1f}% |
| Exact Match Accuracy | {metrics.exact_match_accuracy:.1f}% |
| Avg Inference Time | {metrics.avg_inference_time_ms:.2f} ms |
| Avg TTFT | {metrics.avg_ttft_ms:.2f} ms |

- ‚úÖ {metrics.result_match} correct
- ‚ö†Ô∏è {len(wrong_results)} wrong results  
- ‚ùå {len(syntax_errors)} syntax errors

"""

    # Failures section
    if failures:
        report += "## Failures\n\n"
        
        if syntax_errors:
            report += f"### Syntax Errors ({len(syntax_errors)})\n\n"
            for r in syntax_errors[:5]:
                report += f"""**{r.question_id}**: {r.question}

```sql
-- Gold:
{r.gold_query}

-- Predicted:
{r.predicted_query}
```
Error: `{r.syntax_error}`

---

"""

        if wrong_results:
            report += f"### Wrong Results ({len(wrong_results)})\n\n"
            for r in wrong_results[:5]:
                report += f"""**{r.question_id}**: {r.question}

```sql
-- Gold:
{r.gold_query}

-- Predicted:
{r.predicted_query}
```
Expected {r.gold_row_count} rows, got {r.predicted_row_count}

---

"""

    # Successes
    successes = [r for r in results if r.is_correct()][:3]
    if successes:
        report += "## Sample Successes\n\n"
        for r in successes:
            report += f"""**{r.question_id}**: {r.question}

```sql
{r.predicted_query}
```

---

"""

    report += f"\n*Generated by OT1-APITS Evaluation Framework*\n"
    
    return report
